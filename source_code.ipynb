{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100, 100, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def inv_sigmoid(y):\n",
    "    return np.log(y/(1-y))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = 'Abstract_gallery'\n",
    "os.getcwd()\n",
    "img_list = os.listdir(path)\n",
    "\n",
    "def access_images(img_list,path,length):\n",
    "    pixels = []\n",
    "    imgs = []\n",
    "    for i in range(length):\n",
    "        img = Image.open(path+\"\\\\\"+img_list[i],'r')\n",
    "        basewidth = 100\n",
    "        img = img.resize((basewidth,basewidth),Image.ANTIALIAS)\n",
    "        pix = np.array(img.getdata())\n",
    "        pixels.append(pix.reshape(100,100,3))\n",
    "        imgs.append(img)\n",
    "        \n",
    "    return np.array(pixels),imgs\n",
    "\n",
    "def show_image(pix_list):\n",
    "    array = np.array(pix_list.reshape(100,100,3), dtype = np.uint8)\n",
    "    new_image = Image.fromarray(array)\n",
    "    new_image.show()\n",
    "    \n",
    "pixels,imgs = access_images(img_list,path,1000)\n",
    "pixels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining discriminator\n",
    "This discriminator takes in the list of fake and real images as input and returns a single value between 0 and 1. 0 tends to real and 1 tends to fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape = (100,100,3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    n_nodes = 128 * 25 * 25\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((25, 25, 128)))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(3, (7,7) , padding='same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    d_model.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = g_model.predict(x_input)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=10):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    print(dataset.shape[0])\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "            clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[239 238 236]\n",
      "   [243 242 240]\n",
      "   [241 241 239]\n",
      "   ...\n",
      "   [239 237 238]\n",
      "   [239 237 238]\n",
      "   [239 237 238]]\n",
      "\n",
      "  [[238 236 234]\n",
      "   [220 218 216]\n",
      "   [221 219 218]\n",
      "   ...\n",
      "   [161 160 160]\n",
      "   [175 174 175]\n",
      "   [239 237 238]]\n",
      "\n",
      "  [[225 222 220]\n",
      "   [143 140 139]\n",
      "   [161 159 157]\n",
      "   ...\n",
      "   [160 160 160]\n",
      "   [176 176 177]\n",
      "   [240 238 240]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[247 246 244]\n",
      "   [219 218 216]\n",
      "   [140 139 137]\n",
      "   ...\n",
      "   [139 139 139]\n",
      "   [161 161 161]\n",
      "   [211 210 209]]\n",
      "\n",
      "  [[248 247 245]\n",
      "   [214 213 211]\n",
      "   [109 108 106]\n",
      "   ...\n",
      "   [163 163 163]\n",
      "   [169 169 169]\n",
      "   [209 208 207]]\n",
      "\n",
      "  [[243 243 241]\n",
      "   [232 231 229]\n",
      "   [190 189 187]\n",
      "   ...\n",
      "   [244 244 244]\n",
      "   [240 240 240]\n",
      "   [239 237 238]]]\n",
      "\n",
      "\n",
      " [[[248 248 246]\n",
      "   [247 247 245]\n",
      "   [249 249 247]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [249 249 246]]\n",
      "\n",
      "  [[248 248 246]\n",
      "   [248 248 246]\n",
      "   [237 237 235]\n",
      "   ...\n",
      "   [107 105 103]\n",
      "   [170 168 167]\n",
      "   [253 253 252]]\n",
      "\n",
      "  [[246 245 244]\n",
      "   [255 255 255]\n",
      "   [146 144 139]\n",
      "   ...\n",
      "   [ 22  21  19]\n",
      "   [100  98  96]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[251 251 249]\n",
      "   [254 254 252]\n",
      "   [243 242 240]\n",
      "   ...\n",
      "   [ 98  95  91]\n",
      "   [ 97  90  84]\n",
      "   [206 204 199]]\n",
      "\n",
      "  [[252 252 250]\n",
      "   [252 252 250]\n",
      "   [251 251 249]\n",
      "   ...\n",
      "   [242 242 240]\n",
      "   [244 243 241]\n",
      "   [251 251 249]]\n",
      "\n",
      "  [[253 253 251]\n",
      "   [252 252 250]\n",
      "   [252 252 250]\n",
      "   ...\n",
      "   [255 255 253]\n",
      "   [255 255 253]\n",
      "   [253 253 251]]]\n",
      "\n",
      "\n",
      " [[[ 73  73  73]\n",
      "   [ 33  33  33]\n",
      "   [ 41  41  41]\n",
      "   ...\n",
      "   [ 99  99  99]\n",
      "   [ 96  96  96]\n",
      "   [101 101 101]]\n",
      "\n",
      "  [[ 71  71  71]\n",
      "   [ 32  32  32]\n",
      "   [ 45  45  45]\n",
      "   ...\n",
      "   [ 67  67  67]\n",
      "   [ 68  68  68]\n",
      "   [ 52  52  52]]\n",
      "\n",
      "  [[ 66  66  66]\n",
      "   [ 28  28  28]\n",
      "   [ 31  31  31]\n",
      "   ...\n",
      "   [ 73  73  73]\n",
      "   [ 67  67  67]\n",
      "   [ 59  59  59]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 27  27  27]\n",
      "   [ 24  24  24]\n",
      "   [ 24  24  24]\n",
      "   ...\n",
      "   [ 15  15  15]\n",
      "   [ 18  18  18]\n",
      "   [ 22  22  22]]\n",
      "\n",
      "  [[ 30  30  30]\n",
      "   [ 22  22  22]\n",
      "   [ 23  23  23]\n",
      "   ...\n",
      "   [ 13  13  13]\n",
      "   [ 20  20  20]\n",
      "   [ 21  21  21]]\n",
      "\n",
      "  [[ 30  30  30]\n",
      "   [ 33  33  33]\n",
      "   [ 31  31  31]\n",
      "   ...\n",
      "   [ 15  15  15]\n",
      "   [ 15  15  15]\n",
      "   [ 18  18  18]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 32  32  32]\n",
      "   [ 15  15  15]\n",
      "   [ 16  16  16]\n",
      "   ...\n",
      "   [ 95  95  95]\n",
      "   [ 77  77  77]\n",
      "   [ 42  42  42]]\n",
      "\n",
      "  [[ 49  49  49]\n",
      "   [ 76  76  76]\n",
      "   [ 94  94  94]\n",
      "   ...\n",
      "   [150 150 150]\n",
      "   [135 135 135]\n",
      "   [ 72  72  72]]\n",
      "\n",
      "  [[ 63  63  63]\n",
      "   [ 76  76  76]\n",
      "   [103 103 103]\n",
      "   ...\n",
      "   [154 154 154]\n",
      "   [152 152 152]\n",
      "   [110 110 110]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[178 178 178]\n",
      "   [138 138 138]\n",
      "   [125 125 125]\n",
      "   ...\n",
      "   [125 125 125]\n",
      "   [156 156 156]\n",
      "   [130 130 130]]\n",
      "\n",
      "  [[176 176 176]\n",
      "   [156 156 156]\n",
      "   [138 138 138]\n",
      "   ...\n",
      "   [139 139 139]\n",
      "   [162 162 162]\n",
      "   [145 145 145]]\n",
      "\n",
      "  [[170 170 170]\n",
      "   [161 161 161]\n",
      "   [158 158 158]\n",
      "   ...\n",
      "   [151 151 151]\n",
      "   [162 162 162]\n",
      "   [151 151 151]]]\n",
      "\n",
      "\n",
      " [[[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   1]\n",
      "   [ 28  27  28]\n",
      "   [160 162 135]\n",
      "   ...\n",
      "   [ 71  73  65]\n",
      "   [  0   0   0]\n",
      "   [  3   2   5]]\n",
      "\n",
      "  [[  0   1   2]\n",
      "   [ 19  20  24]\n",
      "   [112 114 101]\n",
      "   ...\n",
      "   [ 23  24  20]\n",
      "   [  0   0   0]\n",
      "   [  1   1   2]]\n",
      "\n",
      "  [[  1   1   2]\n",
      "   [  0   0   1]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]\n",
      "\n",
      "\n",
      " [[[  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   ...\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]]\n",
      "\n",
      "  [[  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   ...\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]]\n",
      "\n",
      "  [[  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   ...\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   ...\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]]\n",
      "\n",
      "  [[  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   ...\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]]\n",
      "\n",
      "  [[  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   ...\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]\n",
      "   [  4   3   8]]]]\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "d_model = define_discriminator()\n",
    "g_model = define_generator(latent_dim)\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "print(pixels.shape)\n",
    "train(g_model, d_model, gan_model, np.array(pixels), latent_dim)\n",
    "print(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference (or actual usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "model = g_model\n",
    "latent_points = generate_latent_points(100,1)\n",
    "X = model.predict(latent_points)\n",
    "\n",
    "array = np.array(X.reshape(100,100,3), dtype=np.uint8)\n",
    "new_image = Image.fromarray(array)\n",
    "new_image = new_image.resize((400,400))\n",
    "new_image.show()\n",
    "new_image.save(\"output.jpg\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
